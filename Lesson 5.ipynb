{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lesson 5\n",
    "\n",
    "\n",
    "Things we will learn in this lesson:\n",
    "\n",
    "    - How to use softmax function for multi-class classifiction\n",
    "    - learn how to use the inbuilt dataset from torch\n",
    "    - Build simple neural network using pytorch\n",
    "    - customize neural network build for single, multiple dimensions and also for multiple output\n",
    "    - exploring multiple activation function and their drawbacks and solutions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftMax function is a function which finds the most probable class among the multiple classes available for classification\n",
    "\n",
    "> **y = argmax{w*x +b}**\n",
    "\n",
    "The output for each class (in a multi class setup)\n",
    "    \n",
    "    - is binary case the output is between 0 and 1 in effect to sigmoid function\n",
    "    - where as in a multi class set up, there is no sigmid function so outputs are in diff scale and it's therfore important to bring all the class probabilities into one scale for comparison\n",
    "    - once the values are brought into scale they can now be comapred and argmax can be performed to choose the most probable class to the particular sample under test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "class softmax(nn.Module):\n",
    "    def __init__(self,in_,out_):\n",
    "        super(softmax,self).__init__()\n",
    "        self.Linear = nn.Linear(in_,out_)\n",
    "    def forward(self,x):\n",
    "        return self.Linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-3,3,0.2)\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = f+(0.1*torch.rand(x.shape[0],1)).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Linear.weight', tensor([[ 0.9640],\n",
       "                      [ 0.6772],\n",
       "                      [-0.1357]])),\n",
       "             ('Linear.bias', tensor([ 0.1914, -0.0856, -0.5479]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0000, -2.8000, -2.6000, -2.4000, -2.2000, -2.0000, -1.8000, -1.6000,\n",
       "        -1.4000, -1.2000, -1.0000, -0.8000, -0.6000, -0.4000, -0.2000,  0.0000,\n",
       "         0.2000,  0.4000,  0.6000,  0.8000,  1.0000,  1.2000,  1.4000,  1.6000,\n",
       "         1.8000,  2.0000,  2.2000,  2.4000,  2.6000,  2.8000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.2666, -2.1496, -2.0327, -1.9157, -1.7988, -1.6818, -1.5578, -1.4061,\n",
       "         -1.2545, -1.1028, -0.9512, -0.7995, -0.6479, -0.4962, -0.3446, -0.1929,\n",
       "         -0.0413,  0.1104,  0.2621,  0.4137,  0.5654,  0.7170,  0.8687,  1.0203,\n",
       "          1.1720,  1.3236,  1.4753,  1.6269,  1.7786,  1.9302],\n",
       "        grad_fn=<MaxBackward0>),\n",
       " tensor([2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = softmax(1,3)\n",
    "yhat = model.forward(x.view(-1,1))\n",
    "\n",
    "##argmax of the predicted probabilities(shown without optimization just as an example)\n",
    "yhat.max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use softmax function along with the loss we will us e special function\n",
    "\n",
    "**nn.CrossEntropyLoss --> this method does Entropy loss and apply softmax function on top**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to donwload a dataset from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                      | 0/9912422 [00:01<?, ?it/s]\u001b[A\n",
      "  0%|▏                                                                      | 24576/9912422 [00:01<01:57, 83855.10it/s]\u001b[A\n",
      "  0%|▎                                                                      | 40960/9912422 [00:01<02:18, 71181.97it/s]\u001b[A\n",
      "  0%|▎                                                                      | 49152/9912422 [00:01<02:34, 63933.48it/s]\u001b[A\n",
      "  1%|▍                                                                      | 57344/9912422 [00:02<02:40, 61414.14it/s]\u001b[A\n",
      "  1%|▌                                                                      | 73728/9912422 [00:02<02:47, 58714.90it/s]\u001b[A\n",
      "  1%|▌                                                                      | 81920/9912422 [00:02<02:58, 55151.24it/s]\u001b[A\n",
      "  1%|▊                                                                     | 114688/9912422 [00:03<02:58, 54787.87it/s]\u001b[A\n",
      "  1%|▊                                                                     | 122880/9912422 [00:03<03:55, 41648.67it/s]\u001b[A\n",
      "  1%|█                                                                     | 147456/9912422 [00:03<03:09, 51568.32it/s]\u001b[A\n",
      "  2%|█▏                                                                    | 163840/9912422 [00:03<02:30, 64710.68it/s]\u001b[A\n",
      "  2%|█▎                                                                    | 180224/9912422 [00:03<02:04, 78124.04it/s]\u001b[A\n",
      "  2%|█▍                                                                    | 196608/9912422 [00:03<01:48, 89925.05it/s]\u001b[A\n",
      "  2%|█▍                                                                   | 212992/9912422 [00:04<01:34, 102405.46it/s]\u001b[A\n",
      "  2%|█▋                                                                   | 237568/9912422 [00:04<01:23, 116449.41it/s]\u001b[A\n",
      "  3%|█▊                                                                   | 262144/9912422 [00:04<01:13, 130704.75it/s]\u001b[A\n",
      "  3%|█▉                                                                   | 286720/9912422 [00:04<01:06, 145770.44it/s]\u001b[A\n",
      "  3%|██▏                                                                  | 311296/9912422 [00:04<01:00, 159968.61it/s]\u001b[A\n",
      "  3%|██▎                                                                  | 335872/9912422 [00:04<00:55, 173745.38it/s]\u001b[A\n",
      "  4%|██▌                                                                  | 368640/9912422 [00:04<00:50, 190163.38it/s]\u001b[A\n",
      "  4%|██▊                                                                  | 401408/9912422 [00:04<00:46, 206088.46it/s]\u001b[A\n",
      "  4%|███                                                                  | 434176/9912422 [00:05<00:42, 224494.26it/s]\u001b[A\n",
      "  5%|███▎                                                                 | 466944/9912422 [00:05<00:39, 241080.53it/s]\u001b[A\n",
      "  5%|███▍                                                                 | 499712/9912422 [00:05<00:35, 261604.59it/s]\u001b[A\n",
      "  5%|███▋                                                                 | 532480/9912422 [00:05<00:33, 277891.91it/s]\u001b[A\n",
      "  6%|███▉                                                                 | 573440/9912422 [00:05<00:30, 301594.18it/s]\u001b[A\n",
      "  6%|████▎                                                                | 614400/9912422 [00:05<00:28, 322262.81it/s]\u001b[A\n",
      "  7%|████▌                                                                | 655360/9912422 [00:05<00:27, 338332.56it/s]\u001b[A\n",
      "  7%|████▉                                                                | 704512/9912422 [00:05<00:25, 366692.89it/s]\u001b[A\n",
      "  8%|█████▏                                                               | 753664/9912422 [00:05<00:23, 389068.73it/s]\u001b[A\n",
      "  8%|█████▋                                                               | 811008/9912422 [00:06<00:21, 414674.99it/s]\u001b[A\n",
      "  9%|██████                                                               | 868352/9912422 [00:06<00:20, 447637.51it/s]\u001b[A\n",
      "  9%|██████▍                                                              | 925696/9912422 [00:06<00:18, 475326.84it/s]\u001b[A\n",
      " 10%|██████▉                                                              | 991232/9912422 [00:06<00:17, 507569.96it/s]\u001b[A\n",
      " 11%|███████▏                                                            | 1056768/9912422 [00:06<00:16, 542985.99it/s]\u001b[A\n",
      " 11%|███████▊                                                            | 1130496/9912422 [00:06<00:15, 574637.48it/s]\u001b[A\n",
      " 12%|████████▎                                                           | 1204224/9912422 [00:06<00:14, 612030.83it/s]\u001b[A\n",
      " 13%|████████▊                                                           | 1286144/9912422 [00:06<00:13, 659943.52it/s]\u001b[A\n",
      " 14%|█████████▍                                                          | 1376256/9912422 [00:06<00:12, 705609.22it/s]\u001b[A\n",
      " 15%|██████████                                                          | 1466368/9912422 [00:06<00:11, 752392.50it/s]\u001b[A\n",
      " 16%|██████████▋                                                         | 1564672/9912422 [00:07<00:10, 801945.30it/s]\u001b[A\n",
      " 17%|███████████▍                                                        | 1671168/9912422 [00:07<00:09, 856241.38it/s]\u001b[A\n",
      " 18%|████████████▏                                                       | 1777664/9912422 [00:07<00:08, 908097.90it/s]\u001b[A\n",
      " 19%|█████████████                                                       | 1900544/9912422 [00:07<00:08, 976495.97it/s]\u001b[A\n",
      " 20%|█████████████▋                                                     | 2023424/9912422 [00:07<00:07, 1038612.74it/s]\u001b[A\n",
      " 22%|██████████████▌                                                    | 2162688/9912422 [00:07<00:06, 1110559.47it/s]\u001b[A\n",
      " 23%|███████████████▌                                                   | 2310144/9912422 [00:07<00:06, 1184362.34it/s]\u001b[A\n",
      " 25%|████████████████▋                                                  | 2465792/9912422 [00:07<00:05, 1271495.87it/s]\u001b[A\n",
      " 27%|█████████████████▊                                                 | 2629632/9912422 [00:07<00:05, 1353766.63it/s]\u001b[A\n",
      " 28%|██████████████████▉                                                | 2801664/9912422 [00:08<00:04, 1434998.71it/s]\u001b[A\n",
      " 30%|████████████████████▎                                              | 2998272/9912422 [00:08<00:04, 1547664.47it/s]\u001b[A\n",
      " 32%|█████████████████████▌                                             | 3194880/9912422 [00:08<00:04, 1652127.51it/s]\u001b[A\n",
      " 34%|███████████████████████                                            | 3407872/9912422 [00:08<00:03, 1764737.27it/s]\u001b[A\n",
      " 36%|████████████████████████▎                                          | 3596288/9912422 [00:08<00:03, 1711913.45it/s]\u001b[A\n",
      " 38%|█████████████████████████▋                                         | 3792896/9912422 [00:08<00:03, 1766733.87it/s]\u001b[A\n",
      " 40%|███████████████████████████                                        | 3997696/9912422 [00:08<00:03, 1837916.67it/s]\u001b[A\n",
      " 42%|████████████████████████████▍                                      | 4202496/9912422 [00:08<00:03, 1893208.61it/s]\u001b[A\n",
      " 44%|█████████████████████████████▊                                     | 4407296/9912422 [00:08<00:02, 1919416.51it/s]\u001b[A\n",
      " 47%|███████████████████████████████▏                                   | 4612096/9912422 [00:08<00:02, 1939949.75it/s]\u001b[A\n",
      " 49%|████████████████████████████████▌                                  | 4816896/9912422 [00:09<00:02, 1953018.56it/s]\u001b[A\n",
      " 51%|█████████████████████████████████▉                                 | 5021696/9912422 [00:09<00:02, 1976223.00it/s]\u001b[A\n",
      " 53%|███████████████████████████████████▎                               | 5226496/9912422 [00:09<00:02, 1978939.98it/s]\u001b[A\n",
      " 55%|████████████████████████████████████▋                              | 5431296/9912422 [00:09<00:02, 1908034.71it/s]\u001b[A\n",
      " 57%|██████████████████████████████████████▏                            | 5644288/9912422 [00:09<00:02, 1962453.69it/s]\u001b[A\n",
      " 59%|███████████████████████████████████████▌                           | 5849088/9912422 [00:09<00:02, 1924151.25it/s]\u001b[A\n",
      " 61%|█████████████████████████████████████████▏                         | 6086656/9912422 [00:09<00:01, 2034191.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████▌                        | 6299648/9912422 [00:09<00:01, 2023162.39it/s]\u001b[A\n",
      " 66%|███████████████████████████████████████████▉                       | 6504448/9912422 [00:09<00:01, 1950021.48it/s]\u001b[A\n",
      " 68%|█████████████████████████████████████████████▎                     | 6709248/9912422 [00:10<00:01, 1938779.25it/s]\u001b[A\n",
      " 70%|██████████████████████████████████████████████▉                    | 6938624/9912422 [00:10<00:01, 2024709.85it/s]\u001b[A\n",
      " 73%|████████████████████████████████████████████████▋                  | 7200768/9912422 [00:10<00:01, 2163457.48it/s]\u001b[A\n",
      " 75%|██████████████████████████████████████████████████▏                | 7421952/9912422 [00:10<00:01, 2149669.96it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████▋               | 7651328/9912422 [00:10<00:01, 2167833.43it/s]\u001b[A\n",
      " 79%|█████████████████████████████████████████████████████▏             | 7872512/9912422 [00:10<00:00, 2169761.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████████████████████████████▋            | 8093696/9912422 [00:10<00:00, 2168324.67it/s]\u001b[A\n",
      " 84%|████████████████████████████████████████████████████████▏          | 8314880/9912422 [00:10<00:00, 2170436.73it/s]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████▋         | 8536064/9912422 [00:10<00:00, 2167024.39it/s]\u001b[A\n",
      " 88%|███████████████████████████████████████████████████████████▏       | 8757248/9912422 [00:10<00:00, 2165483.40it/s]\u001b[A\n",
      " 91%|████████████████████████████████████████████████████████████▋      | 8978432/9912422 [00:11<00:00, 2169217.42it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████████████████████████████▏    | 9199616/9912422 [00:11<00:00, 2170244.59it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████████████████████████████▋   | 9420800/9912422 [00:11<00:00, 2170874.26it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████████████████████████████▏ | 9641984/9912422 [00:11<00:00, 2168309.88it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████▋| 9863168/9912422 [00:11<00:00, 2177235.66it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                        | 0/28881 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|█████████████████████████████████████████████████████████████▎          | 24576/28881 [00:00<00:00, 138363.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                      | 0/1648877 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|█                                                                     | 24576/1648877 [00:00<00:13, 120562.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|█▍                                                                     | 32768/1648877 [00:01<00:17, 93658.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|██                                                                     | 49152/1648877 [00:01<00:19, 81680.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|██▊                                                                    | 65536/1648877 [00:01<00:17, 91264.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|████▍                                                                | 106496/1648877 [00:01<00:13, 117243.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|█████▍                                                               | 131072/1648877 [00:01<00:11, 137620.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|████████▏                                                            | 196608/1648877 [00:01<00:08, 178672.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|██████████▋                                                          | 253952/1648877 [00:01<00:06, 223398.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██████████████                                                       | 335872/1648877 [00:01<00:04, 285257.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|███████████████████▌                                                 | 466944/1648877 [00:02<00:03, 371670.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████████████████▋                                          | 638976/1648877 [00:02<00:02, 485247.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████████████████████▋                                 | 851968/1648877 [00:02<00:01, 630364.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████████████████████████▉                    | 1163264/1648877 [00:02<00:00, 827976.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "1654784it [00:02, 628402.26it/s]                                                                                       \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "8192it [00:00, 10831.75it/s]                                                                                           \u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train = dsets.MNIST(root='./', train=True, download=True, transform=transforms.ToTensor())\n",
    "validation = dsets.MNIST(root='./', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train, batch_size=100)\n",
    "validation_loader = DataLoader(dataset = train, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below piece of code iterates over the number of epoch\n",
    "\n",
    "For every epoch,\n",
    "\n",
    "    -train dataset is sent in batches specified in dataloader method\n",
    "    -loss, backpropagation and weight updation takes place\n",
    "    -after which validation dataset is validation on the trained model and their correposning accuracy metric is calculated\n",
    "    -this process is repeated for specified n_epoch(10 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for x,y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model.forward(x.view(-1,28*28))\n",
    "        loss_ = loss(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    correct_prediction = 0\n",
    "    \n",
    "    for x_test,y_test in validation_loader:\n",
    "        z = model(x_test.view(-1,28*28))\n",
    "        _,yhat = torch.max(z.data,1)\n",
    "        correct_prediction = correct_prediction + (yhat == y_test).sum().item()\n",
    "\n",
    "    accuracy = correct_prediction/len(x_test)\n",
    "    accuracy_list.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with multi-dimensonal input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,D_in, H, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.Linear1 = nn.Linear(D_in, H)\n",
    "        self.Linear2 = nn.Linear(H, D_out)\n",
    "    def forward_(self,x):\n",
    "        x = torch.sigmoid(self.Linear1(x))\n",
    "        x = torch.sigmoid(self.Linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(1,2,1)\n",
    "\n",
    "error = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr =0.01)\n",
    "x = torch.tensor([-3.0])\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = torch.tensor(-10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost at the end of epoch0 is 2.75771164894104\n",
      "The cost at the end of epoch1 is -0.09584802389144897\n",
      "The cost at the end of epoch2 is -2.9469614028930664\n",
      "The cost at the end of epoch3 is -5.797658443450928\n",
      "The cost at the end of epoch4 is -8.649755477905273\n",
      "The cost at the end of epoch5 is -11.504983901977539\n",
      "The cost at the end of epoch6 is -14.364992141723633\n",
      "The cost at the end of epoch7 is -17.23130226135254\n",
      "The cost at the end of epoch8 is -20.10519027709961\n",
      "The cost at the end of epoch9 is -22.987611770629883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:2016: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "loss_ = []\n",
    "for epoch in range(10):\n",
    "    yhat = model.forward_(x)\n",
    "    loss = error(yhat, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_.append(loss.item())\n",
    "    optimizer.step()\n",
    "    print('The cost at the end of epoch{} is {}'.format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could claearly see how a neural network id getting built\n",
    "\n",
    "Summary:\n",
    "    - for a single scalar input with one sample, we create a 2 linear models in the hidden layer (hence Linear1(1,2))\n",
    "    - once we built the Linear1 model which outputs two outputs, we pass them through the sigmoid function to add non-linearity\n",
    "    - These output values are fed to built one linear model (hence Linear2(2,1))\n",
    "    - The output is passed through sigmoid to have the final predictions\n",
    "    - the error/loss is then calculated using Binary Cross Entropy \n",
    "    - weights are adjusted using differntiation\n",
    "    - this process is repeated for n epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting:\n",
    "    - this occurs when the model is complex than the data (i.e model is built very complex that it learns the entire data by-heart)\n",
    "    - obviously the model so built has too many neurons\n",
    "    \n",
    "### Underfitting:\n",
    "    - this occurs when a data is more complex than the model built(doesnot learn the basics of the data representation)\n",
    "    - cause could be very few neurons\n",
    "    - very less data\n",
    "\n",
    "**Solution:**\n",
    "    - use validation data to identify the optimal number of neurons for the specific dataset\n",
    "    - regularization(dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with multi-dimensonal input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,D_in, H, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.Linear1 = nn.Linear(D_in, H)\n",
    "        self.Linear2 = nn.Linear(H, D_out)\n",
    "    def forward_(self,x):\n",
    "        x = torch.sigmoid(self.Linear1(x))\n",
    "        x = torch.sigmoid(self.Linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a single sample input with 3 dimensions and its out target as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Net(3,6,1)\n",
    "\n",
    "error = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr =0.01)\n",
    "x = torch.tensor([-3.0,10.0,5.0])\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = torch.tensor([1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost at the end of epoch0 is 0.45546144247055054\n",
      "The cost at the end of epoch10 is 0.41135525703430176\n",
      "The cost at the end of epoch20 is 0.3729822039604187\n",
      "The cost at the end of epoch30 is 0.3397429883480072\n",
      "The cost at the end of epoch40 is 0.3108556866645813\n",
      "The cost at the end of epoch50 is 0.28562384843826294\n",
      "The cost at the end of epoch60 is 0.2634837031364441\n",
      "The cost at the end of epoch70 is 0.24398145079612732\n",
      "The cost at the end of epoch80 is 0.2267438769340515\n",
      "The cost at the end of epoch90 is 0.21145708858966827\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "loss_ = []\n",
    "for epoch in range(100):\n",
    "    yhat = model.forward_(x)\n",
    "    loss = error(yhat, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_.append(loss.item())\n",
    "    optimizer.step()\n",
    "    if epoch%10==0:\n",
    "        print('The cost at the end of epoch{} is {}'.format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8194], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is close to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with muti-class output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,D_in, H, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.Linear1 = nn.Linear(D_in, H)\n",
    "        self.Linear2 = nn.Linear(H, D_out)\n",
    "    def forward_(self,x):\n",
    "        x = torch.sigmoid(self.Linear1(x))\n",
    "        x = self.Linear2(x) #**remove sigmoid isnce we are applying softmax for multiclass**\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(3,6,2)\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr =0.01)\n",
    "x = torch.tensor([[-3.0,10.0,5.0],[-1.0,0.0,-5.0],[-2.0,10.0,5.0]])\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = torch.tensor([[0.],[1.],[0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = []\n",
    "loss_ = []\n",
    "for epoch in range(100):\n",
    "    yhat = model.forward_(x)\n",
    "    print(yhat)\n",
    "    print(y)\n",
    "    loss = error(y, yhat) #cross entrpy loss takes in only long int format **caution\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_.append(loss.item())\n",
    "    optimizer.step()\n",
    "    if epoch%10==0:\n",
    "        print('The cost at the end of epoch{} is {}'.format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4629, -0.0914],\n",
       "        [ 0.3035,  0.0968],\n",
       "        [ 0.4620, -0.0798]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular activation functions are:\n",
    "    \n",
    "    - sigmoid (0 to 1 but serious disadvantage of vanishing gradient descent)\n",
    "    - tanh (-1 to +1)\n",
    "    - Relu activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid activation function with derivative shows that,\n",
    "\n",
    "the derivating does increase for a moment upto 2.0 but decreases down to zero for higher values of z\n",
    "\n",
    "Hence suffers **Vanishing gradient Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./sigmoid.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how sigmoid is used in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetRelu(nn.Module):    \n",
    "        def __init__(self,D_in,H,D_out):\n",
    "            super(NetRelu,self).__init__() \n",
    "            self.linear1=nn.Linear(D_in,H)        \n",
    "            self.linear2=nn.Linear(H,D_out)            \n",
    "        def forward(self,x):        \n",
    "            x=torch.sigmoid(self.linear1(x))          \n",
    "            x=torch.sigmoid(self.linear2(x))     \n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tanh activation function with derivative shows that,\n",
    "\n",
    "the derivating does increase for a moment upto 1.0 but decreases down to zero for higher values of z\n",
    "\n",
    "Hence suffers **Vanishing gradient Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./tanh.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetRelu(nn.Module):    \n",
    "        def __init__(self,D_in,H,D_out):\n",
    "            super(NetRelu,self).__init__() \n",
    "            self.linear1=nn.Linear(D_in,H)        \n",
    "            self.linear2=nn.Linear(H,D_out)            \n",
    "        def forward(self,x):        \n",
    "            x=torch.tanh(self.linear1(x))          \n",
    "            x=self.linear2(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relu activation function with derivative shows that,\n",
    "\n",
    "the derivative goes to constant 1.0 for all positve values of z\n",
    "\n",
    "**Vanishing gradient Problem** has a solution NOW!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./relu.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetRelu(nn.Module):    \n",
    "        def __init__(self,D_in,H,D_out):\n",
    "            super(NetRelu,self).__init__() \n",
    "            self.linear1=nn.Linear(D_in,H)        \n",
    "            self.linear2=nn.Linear(H,D_out)            \n",
    "        def forward(self,x):        \n",
    "            x=torch.relu(self.linear1(x))          \n",
    "            x=self.linear2(x)\n",
    "            return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
